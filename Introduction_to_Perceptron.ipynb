{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnaHauschild/Artificial-Intelligence/blob/main/Introduction_to_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPXFHtR67XSz"
      },
      "source": [
        "# Introduction to Neural Networks\n",
        "\n",
        "\n",
        "** Ecole Centrale Nantes **\n",
        "\n",
        "** Diana Mateus **\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaP5KNXD7XS3"
      },
      "source": [
        "** Participants : Anna Hauschild**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-FyZE3B7XS4"
      },
      "source": [
        "## General description\n",
        "In this lab we will create a simple classifier based on neural networks. We will progress in two parts:\n",
        "- In the first part, and to better understand the involved operations, we will create a single-neuron model and optimize its parameters \"by hand\". For this first part we will only use the **Numpy** library\n",
        "- We will then build a multi-layer perceptron with the built-in library **Keras** module and **tensorflow**. Tensorflow is already installed in the university computers. If using your own computer you should have already installed **tensorflow** or use **collab** online platform.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vg8_j5r97XS5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvCSopBH7XS6"
      },
      "source": [
        "### Loading the dataset\n",
        "Start by runing the following lines to load and visualize the data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UNCOMMENT IF USING COLAB\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "IMDIR = ('/content/drive/MyDrive/Colab Notebooks/dataset (1)')"
      ],
      "metadata": {
        "id": "7_B5zGllLX9Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46f43899-52bf-43db-9d20-dda410c789ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Lb_1kDq77XS6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "e3c6c54a-d4bf-4732-9fd0-2329b8740d4f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotADirectoryError",
          "evalue": "[Errno 20] Unable to synchronously open file (unable to open file: name = '/content/drive/MyDrive/Colab Notebooks/dataset (1)/train_catvnoncat.h5', errno = 20, error message = 'Not a directory', flags = 0, o_flags = 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-54af3edb4a1b>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMDIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-54af3edb4a1b>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(IMDIR)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMDIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMDIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/train_catvnoncat.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_set_x\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_set_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMDIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/test_catvnoncat.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    559\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 561\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Unable to synchronously open file (unable to open file: name = '/content/drive/MyDrive/Colab Notebooks/dataset (1)/train_catvnoncat.h5', errno = 20, error message = 'Not a directory', flags = 0, o_flags = 0)"
          ]
        }
      ],
      "source": [
        "def load_dataset(IMDIR):\n",
        "    train_dataset = h5py.File(IMDIR+'/train_catvnoncat.h5', \"r\")\n",
        "    train_x = np.array(train_dataset[\"train_set_x\"][:])\n",
        "    train_y = np.array(train_dataset[\"train_set_y\"][:])\n",
        "    test_dataset = h5py.File(IMDIR+'/test_catvnoncat.h5', \"r\")\n",
        "    test_x = np.array(test_dataset[\"test_set_x\"][:])\n",
        "    test_y = np.array(test_dataset[\"test_set_y\"][:])\n",
        "    classes = np.array(test_dataset[\"list_classes\"][:])\n",
        "\n",
        "    train_y = train_y.reshape((1, train_y.shape[0]))\n",
        "    test_y = test_y.reshape((1, test_y.shape[0]))\n",
        "\n",
        "    return train_x, train_y, test_x, test_y, classes\n",
        "\n",
        "train_x, train_y, test_x, test_y, classes=load_dataset(IMDIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0hXCCuW7XS7"
      },
      "source": [
        "#### Visualize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcGj2XDE7XS7"
      },
      "outputs": [],
      "source": [
        "# run several times to visualize different data points\n",
        "# the title shows the ground truth class labels (0=no cat , 1 = cat)\n",
        "index = np.random.randint(low=0,high=train_y.shape[1])\n",
        "plt.imshow(train_x[index])\n",
        "plt.title(\"Image \"+str(index)+\" label \"+str(train_y[0,index]))\n",
        "plt.show()\n",
        "print (\"Train X shape: \" + str(train_x.shape))\n",
        "print (\"We have \"+str(train_x.shape[0]),\n",
        "       \"images of dimensionality \"\n",
        "       + str(train_x.shape[1])+ \"x\"\n",
        "       + str(train_x.shape[2])+ \"x\"\n",
        "       + str(train_x.shape[3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFV3XzZP7XS8"
      },
      "source": [
        "#### Preprocessing\n",
        "In the following lines we vectorize the images (Instead of a 2-D image we will give as input to the models a 1-D vector). The normalization makes the image intensities be between 0 and 1, and converts the images to floats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SvsLwGb7XS9"
      },
      "outputs": [],
      "source": [
        "train_x, train_y, test_x, test_y, classes=load_dataset(IMDIR)\n",
        "train_x = train_x.reshape(train_x.shape[0], -1).T\n",
        "test_x = test_x.reshape(test_x.shape[0], -1).T\n",
        "print (\"Train X shape: \" + str(train_x.shape))\n",
        "print (\"Train Y shape: \" + str(train_y.shape))\n",
        "print (\"Test X shape: \" + str(test_x.shape))\n",
        "print (\"Test Y shape: \" + str(test_y.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuv_0I7O7XS9"
      },
      "outputs": [],
      "source": [
        "train_x = train_x/255.\n",
        "test_x = test_x/255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsHkFIdd7XS-"
      },
      "source": [
        "### 1. Classification with a single neuron\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-HI-OoM7XS-"
      },
      "source": [
        "**a)** Fill-in the following three functions to define the single neuron model (a single neuron in the hidden layer):\n",
        "- A function **initialize_parameters** of the neuron. The function will randomly initializes the model's weights with small values. Initialize the bias with 0. What is the number of weights required? pass this information as a parameter to the function.\n",
        "- A function **sigmoid** that computes the sigmoid activation function\n",
        "- A function **neuron** that given an input vector, the weights and bias, computes the output of the single neuron model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_qACoB77XS_"
      },
      "outputs": [],
      "source": [
        "\n",
        "def sigmoid(z):\n",
        "    return 1/(1+np.exp(-z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Q4QQDSy7XS_"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(dim):\n",
        "    w = np.random.randn(dim,1)*0.01 #w small for the sigmoid function. It has to be between 0 and one. Since X is not small W has to be. ->multiplication by 0.01\n",
        "    return w, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYIgpzVN7XTA"
      },
      "outputs": [],
      "source": [
        "def neuron(w,b,X):\n",
        "    pred_y = sigmoid(np.matmul(w.T,X) + b) #matmul matrix multiplication\n",
        "    #pred_y = sigmoid(np.dot(X,w) + b)\n",
        "    return pred_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfaqixLE7XTA"
      },
      "source": [
        "**b)** **Forward Pass:**\n",
        "Use the three functions above to compute a first forward pass for the input matrix $X$ containing the loaded dataset, for some initialization of the weights and bias.\n",
        "\n",
        " \\begin{align}\n",
        " Y_{\\rm pred}=\\sigma(w^\\top X+b) = [y_{\\rm pred}^{(1)},y_{\\rm pred}^{(2)},\\dots,y_{\\rm pred}^{(m)}]\n",
        " \\end{align}\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "64x64 for each layer, three layers for red, blue and green.\n",
        "\n",
        "64x64x3 = 12288 (Dimension)\n",
        "\n",
        "209 images\n"
      ],
      "metadata": {
        "id": "qej5TS8JjcbT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFWcskKb7XTA"
      },
      "outputs": [],
      "source": [
        "X = train_x\n",
        "w,b = initialize_parameters(12288)\n",
        "prediction = neuron(w,b,X)\n",
        "print(prediction.shape) #209 images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJMQ61FJ7XTB"
      },
      "source": [
        "**c) Cost estimation:**\n",
        "\n",
        "We will use a binary cross-entropy loss, so that the empirical risk can be computed as:\n",
        " \\begin{align}\n",
        " E = - \\frac{1}{m} \\sum_{i=1}^m\n",
        " y^{(i)} \\log(y_{\\rm pred}^{(i)}) +\n",
        " (1-y^{(i)}) \\log(1-y_{\\rm pred}^{(i)})\n",
        " \\end{align}\n",
        "\n",
        " The following cross-entropy function should give as result the scalar cost value computed over the entire dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quality of the model.\n",
        "\n",
        "First part should be 1 and second part 0. Or the other way round.\n",
        "So for a perfect estimation the crossentropy would be 1. Here we get 0.78.\n"
      ],
      "metadata": {
        "id": "iG0FjkX3ykwG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOfXYhdX7XTB"
      },
      "outputs": [],
      "source": [
        "def crossentropy(Y,Ypred):\n",
        "    m = 209\n",
        "    cost = -1/m * np.sum(Y * np.log(Ypred) + (1-Y) * np.log(1-Ypred))\n",
        "    return cost\n",
        "\n",
        "pred_y = neuron(w,b,X)\n",
        "#pred_y = np.zeros(209)\n",
        "#for image in range(train_x.shape[1]):\n",
        "#  pred_y[image] = neuron(w,b,X[:,image])\n",
        "crossentropy(train_y,pred_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpUjfeFl7XTB"
      },
      "source": [
        "**d) Back propagation:**\n",
        "\n",
        "After initializing the parameters and doing a forward pass, we need to backpropagate the cost by computing the gradient with respect to the model parameters to later update the weights\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial E}{\\partial w} =\n",
        "& \\frac{1}{m} X(Y_{\\rm pred}-Y)^T =\n",
        " \\frac{1}{m} \\sum_{i=1}^m x^{(i)}(y^{(i)}_{\\rm pred}-y^{(i)})\\\\\n",
        "\\frac{\\partial E}{\\partial b} =\n",
        "& \\frac{1}{m} \\sum_{i=1}^m(y^{(i)}_{\\rm pred}-y^{(i)})\\\\\n",
        "\\end{align}\n",
        "\n",
        "See a demonstration of the gradient computation in\n",
        "https://en.wikipedia.org/wiki/Cross_entropy\n",
        "\n",
        "Fill-in the backpropagation function which receives as input the the training set (X,Y), as well as the current predictions and returns the gradients updates for the weights and bias\n",
        "\n",
        "Hint: When the error is computed for several samples simultaneously, the gradient is averaged over the contribution of different samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnLozAFY7XTC"
      },
      "outputs": [],
      "source": [
        "def backpropagate(X, Y, Ypred):\n",
        "    m = X.shape[1]\n",
        "\n",
        "    #find gradient (back propagation)\n",
        "    dw = 1/m * np.matmul(X, (Ypred-Y).T)\n",
        "    db = 1/m * np.sum(Ypred-Y)\n",
        "    grads = {\"dw\": dw, #dictionary with key \"dw\"\n",
        "             \"db\": db} #change of bias\n",
        "\n",
        "    return grads\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYIlVxrX7XTC"
      },
      "source": [
        "**e) Optimization**\n",
        "After initializing the parameters, computing the cost function, and calculating gradients, we can now update the parameters using gradient descent. Use the functions implemented above to fill_in the \"gradient_descent\" function that optimizes the parameters given a training set X, Y, a fixed number of iterations, and a learning_rate. Store and plot the value of the loss function at each iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBSR8UOU7XTC"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, Y, iterations, learning_rate):\n",
        "    costs = []\n",
        "    w, b = initialize_parameters(train_x.shape[0])\n",
        "\n",
        "    for i in range(iterations):\n",
        "        Ypred = neuron(w,b,X)\n",
        "        cost = crossentropy(Y,Ypred)\n",
        "        grads= backpropagate(X, Y, Ypred)\n",
        "\n",
        "        #update parameters\n",
        "        w = w - learning_rate * grads[\"dw\"]\n",
        "        b = b - learning_rate * grads[\"db\"]\n",
        "        costs.append(cost)\n",
        "        #print(w.shape)\n",
        "        if i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "    return w,b, costs\n",
        "\n",
        "w, b, costs = gradient_descent(train_x,train_y,iterations=2000, learning_rate = 0.005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOfvYu8m7XTD"
      },
      "source": [
        "**e) Plot the training curve**\n",
        "Plot the evolution of the cost vs the iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSlvAuw17XTD"
      },
      "outputs": [],
      "source": [
        "plt.plot(costs)\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U0z7L2e7XTE"
      },
      "source": [
        "**f) Prediction**\n",
        "Use the optimized parameters to make predictions both for the train and test sets and compute the accuracy for each. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fa5P3G0a7XTE"
      },
      "outputs": [],
      "source": [
        "def predict(w, b, X):\n",
        " tmp =neuron(w,b,X)\n",
        " y_pred = np.zeros((X.shape[1],1))\n",
        " for i in range(X.shape[1]):\n",
        "   if tmp[0,i] > 0.5:\n",
        "     y_pred[i,0] = 1\n",
        " return y_pred\n",
        "\n",
        "# predict\n",
        "train_pred_y = predict(w, b, train_x)\n",
        "test_pred_y = predict(w, b, test_x)\n",
        "print(\"Train Acc: {} %\".format(100 - np.mean(np.abs(train_pred_y - train_y)) * 100))\n",
        "print(\"Test Acc: {} %\".format(100 - np.mean(np.abs(test_pred_y - test_y)) * 100))\n",
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tf2",
      "language": "python",
      "name": "tf2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}