{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnaHauschild/Artificial-Intelligence/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si6cLhP42iU6"
      },
      "source": [
        "# Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXlAqS8aV1JG"
      },
      "source": [
        "In this laboratory we continue to work with Keras. We will focus on Convolutional Neural Network\n",
        "we are going to work with cifar10, a  dataset consisting of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "Therefore the main goal of this laboratory is to solve a multiclass classification problem with 10 different classes\n",
        "\n",
        "<br><img src= \"https://drive.google.com/uc?export=view&id=1u-htQUPUm40h4HBBvg-JA5r0IfrRHqrq\" width=\"500px\" align=\"middle\"><br>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7c5MFuE2iU9"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "In this laboratory we work again on the cifar10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqEHTkg7xv1c"
      },
      "outputs": [],
      "source": [
        "# import some libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVAclqsq2iU_"
      },
      "source": [
        "### Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnTmYxnF2iVG"
      },
      "outputs": [],
      "source": [
        "# load the dataset\n",
        "num_classes = 10\n",
        "new_im_size = 32\n",
        "channels = 3\n",
        "cifar10 = tf.keras.datasets.cifar10\n",
        "(x_learn, y_learn),(x_test, y_test) = cifar10.load_data()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "916Qmf_V2iVM"
      },
      "source": [
        "##Â Dataset pre-processing\n",
        "As pre-processing steps we apply\n",
        "\n",
        "* Normalization in [0 1], to speed up learning and have faster convergence\n",
        "* Splitting in training and validation (needed for an appropriate learning pipeline) and label preprocessing (one-hot encoding)\n",
        "* Standardization, to obtain a distribution with mean equal to 0 and a standard deviation equal to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1ar8l3i2iVU",
        "outputId": "9ee1156a-aa38-4c2d-a43e-9056f3f7674a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalizing training set..\n",
            "Normalizing test set..\n"
          ]
        }
      ],
      "source": [
        "# Normalize the data in [0 1]\n",
        "print(\"Normalizing training set..\")\n",
        "x_learn = np.asarray(x_learn, dtype=np.float32) / 255 # Normalizing training set\n",
        "print(\"Normalizing test set..\")\n",
        "x_test = np.asarray(x_test, dtype=np.float32) / 255 # Normalizing test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gx4Pz2Z5yH2q"
      },
      "outputs": [],
      "source": [
        "# split in training and validation\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_learn, y_learn, test_size=0.25, random_state=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMHYFStMapV1"
      },
      "source": [
        "## 2.1.3 Standardization\n",
        "Another common practice in data pre-processing is standardization.<br>\n",
        "The idea about standardization is to compute your dataset mean and standard deviation in order to subtract from every data point $x$ the dataset mean $\\mu$ and then divide by the standard deviation $\\sigma$.<br>\n",
        "That is to apply the following operation:<br>\n",
        "<img src=\"https://drive.google.com/uc?id=1rpuybw_fmI8XK38JQhWWxX2TOExBAV2V\" width=\"150px\"><br>\n",
        "The outcome of this operation is to obtain a distribution with mean equal to 0 and a standard deviation equal to 1.<br>\n",
        "By applying normalization to our data we are making the features more similar to each other and this usually makes the learning process easier.<br>\n",
        "To better understand that we can show an example of what happens after a standardization process is applied to a dataset:\n",
        "<img src=\"https://drive.google.com/uc?id=1wtqTW4hz8n8k7b7q0mUSzCc9X0npNUY2\" width=\"500px\" align=\"left\"><br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdeRE2XsyIo7",
        "outputId": "92b07de9-2d37-4a75-d35b-b4965cd90b09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Standardizing training set..\n",
            "Standardizing validation set..\n",
            "Standardizing test set..\n",
            "Size of the training set\n",
            "x_train (37500, 32, 32, 3)\n",
            "y_train (37500, 1)\n",
            "Size of the validation set\n",
            "x_val (12500, 32, 32, 3)\n",
            "y_val (12500, 1)\n",
            "Size of the test set\n",
            "x_test (10000, 32, 32, 3)\n",
            "y_test (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "# Standardizing the data\n",
        "def standardize_dataset(X):\n",
        "    image_means = []\n",
        "    image_stds = []\n",
        "\n",
        "    for image in X:\n",
        "        image_means.append(np.mean(image)) # Computing the image mean\n",
        "        image_stds.append(np.std(image)) # Computing the image standard deviation\n",
        "\n",
        "    dataset_mean = np.mean(image_means) # Computing the dataset mean\n",
        "    dataset_std = np.mean(image_stds) # Computing the dataset standard deviation\n",
        "    return [dataset_mean, dataset_std] # For every image we subtract to it the dataset mean and we divide by the dataset standard deviation\n",
        "\n",
        "dataset_mean, dataset_std = standardize_dataset(x_train)\n",
        "\n",
        "print(\"Standardizing training set..\")\n",
        "x_train = (x_train-dataset_mean)/dataset_std # Standardizing the training set\n",
        "print(\"Standardizing validation set..\")\n",
        "x_val = (x_val-dataset_mean)/dataset_std # Standardizing the test set\n",
        "print(\"Standardizing test set..\")\n",
        "x_test = (x_test-dataset_mean)/dataset_std # Standardizing the test set\n",
        "\n",
        "# one hot encode target values\n",
        "y_train_enc = tf.keras.utils.to_categorical(y_train)\n",
        "y_val_enc = tf.keras.utils.to_categorical(y_val)\n",
        "y_test_enc = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "print(\"Size of the training set\")\n",
        "print(\"x_train\", x_train.shape)\n",
        "print(\"y_train\", y_train.shape)\n",
        "\n",
        "print(\"Size of the validation set\")\n",
        "print(\"x_val\", x_val.shape)\n",
        "print(\"y_val\", y_val.shape)\n",
        "\n",
        "print(\"Size of the test set\")\n",
        "print(\"x_test\", x_test.shape)\n",
        "print(\"y_test\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLIf2fS62iVn"
      },
      "source": [
        "# 2.2 Training a model from scratch\n",
        "Now that we have properly pre-processed our data, we are going to create a convolutional model in Keras.\n",
        "Usually a convolutional model is made by two subsequent part:\n",
        "* A convolutional part\n",
        "* A fully connected\n",
        "\n",
        "We can show an example of the general structure in the next picture:\n",
        "<img src=\"https://drive.google.com/uc?id=1duP8u9bs6ELNu4degUuYP4-YS1mBYn2O\" width=\"600px\"><br>\n",
        "\n",
        "Usually the convolutional part is made by some layers composed by\n",
        "* convolutional layer: performs a spatial convolution over images\n",
        "* pooling layer: used to reduce the output spatial dimension from $n$ to 1 by averaging the $n$ different value or considering the maximum between them\n",
        "* dropout layer: applied to a layer, consists of randomly \"dropping out\" (i.e. set to zero) a number of output features of the layer during training.\n",
        "\n",
        "The convolutional part produces its output and the fully connected part ties together the received information in order to solve the classification problem.\n",
        "Let us start with a shallow architecture with only 2 conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SQ6lArz2iVo"
      },
      "outputs": [],
      "source": [
        "# Creating the model from scratch\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras import Sequential,Input,Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "scratch_model = Sequential()\n",
        "\n",
        "# Build here your keras model.\n",
        "# Try to use one convolutional layer, joint with pooling layer and dropout layer\n",
        "\n",
        "# Creating conv 1: conv with 1024 kernels of size 3x3, padding='same', input_shape=(new_im_size, new_im_size, channels)\n",
        "# + LeakyReLU(alpha=0.1) + maxpooling with region size 2x2 and padding='same'+Dropout(0.25)\n",
        "scratch_model.add(Conv2D(1024, kernel_size=(3, 3), padding='same',input_shape=(new_im_size, new_im_size, channels)))\n",
        "scratch_model.add(LeakyReLU(alpha=0.1))\n",
        "scratch_model.add(MaxPooling2D((2, 2),padding='same'))\n",
        "scratch_model.add(Dropout(0.25))\n",
        "\n",
        "scratch_model.add(Conv2D(254, kernel_size=(3, 3), padding='same',input_shape=(new_im_size, new_im_size, channels)))\n",
        "scratch_model.add(LeakyReLU(alpha=0.1))\n",
        "scratch_model.add(MaxPooling2D((2, 2),padding='same'))\n",
        "scratch_model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "# Adding the dense final part: Flatten + Dense with 64 neurons and relu + Dropout 25% + Dense with 10 neurons and softmax\n",
        "#...\n",
        "\n",
        "# Compile the model with the Adam optimizer\n",
        "#scratch_model.compile(loss=tensorflow.keras.losses.categorical_crossentropy, optimizer=tensorflow.keras.optimizers.Adam(),metrics=['accuracy'])\n",
        "\n",
        "# Visualize the model through the summary function\n",
        "#scratch_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TDyHTlt13VZ"
      },
      "source": [
        "<img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F070ca1bd-fb35-4ed1-9d49-0e7979535288_675x370.jpeg\" width=\"600px\"><br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrvjrFyz1dIp"
      },
      "outputs": [],
      "source": [
        "# Try to make the netwok deeper, adding more Conv and Pooling layers. Do the performances improve?\n",
        "# ...\n",
        "# ...\n",
        "# ...\n",
        "\n",
        "# Creating conv 1: conv with 1024 kernels of size 3x3, padding='same', input_shape=(new_im_size, new_im_size, channels)\n",
        "# + LeakyReLU(alpha=0.1) + maxpooling with region size 2x2 and padding='same'+Dropout(0.25)\n",
        "scratch_model.add(Conv2D(128, kernel_size=(3, 3), padding='same',input_shape=(new_im_size, new_im_size, channels)))\n",
        "scratch_model.add(LeakyReLU(alpha=0.1))\n",
        "scratch_model.add(MaxPooling2D((2, 2),padding='same'))\n",
        "scratch_model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "# Adding the dense final part: Flatten + Dense with 64 neurons and relu + Dropout 25% + Dense with 10 neurons and softmax\n",
        "#...\n",
        "# Flatten\n",
        "scratch_model.add(Flatten())\n",
        "\n",
        "# Dense with 64 neurons and relu + Dropout 25%\n",
        "scratch_model.add(Dense(64))\n",
        "scratch_model.add(LeakyReLU(alpha=0.1))\n",
        "scratch_model.add(Dropout(0.25))\n",
        "\n",
        "# Dense with 10 neurons and softmax (Output)\n",
        "scratch_model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model with the Adam optimizer\n",
        "scratch_model.compile(loss=tensorflow.keras.losses.categorical_crossentropy, optimizer=tensorflow.keras.optimizers.Adam(),metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "azjgug3Q2iVs",
        "outputId": "9f02b491-2dcc-41d3-823e-ff77923ded5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "586/586 [==============================] - 4242s 7s/step - loss: 1.5346 - accuracy: 0.4425 - val_loss: 1.1841 - val_accuracy: 0.5827\n",
            "Epoch 2/5\n",
            "179/586 [========>.....................] - ETA: 43:42 - loss: 1.1820 - accuracy: 0.5760"
          ]
        }
      ],
      "source": [
        "# Let's train the model!\n",
        "\n",
        "# Network parameters\n",
        "batch_size = 64 # Setting the batch size\n",
        "epochs = 5\n",
        "# On GPU this is fast process\n",
        "scratch_model_history = scratch_model.fit(x_train, y_train_enc, batch_size=batch_size, shuffle=True, epochs=epochs, validation_data=(x_val, y_val_enc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1R2yQaYBu3E_"
      },
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper left')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbPMcLKV2iVy"
      },
      "outputs": [],
      "source": [
        "# Getting the results\n",
        "\n",
        "plot_history(scratch_model_history)\n",
        "\n",
        "print(\"Training accuracy: \", accuracy_score(np.argmax(scratch_model.predict(x_train), axis=-1), y_train))\n",
        "print(\"Validation accuracy: \", accuracy_score(np.argmax(scratch_model.predict(x_val), axis=-1), y_val))\n",
        "print(\"Test accuracy: \", accuracy_score(np.argmax(scratch_model.predict(x_test), axis=-1), y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeXA6F6UmZsW"
      },
      "source": [
        "### Add data augmentation, using for instance:\n",
        "\n",
        "\n",
        "> data_augmentation = tf.keras.Sequential([tensorflow.keras.layers.RandomFlip(\"horizontal_and_vertical\"), tensorflow.keras.layers.RandomRotation(0.2),])\n",
        "\n",
        "\n",
        "> model.add(data_augmentation)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Try to make the network deeper, adding more Conv and Pooling layers. Do the performances improve?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3_XbZQg2iWQ"
      },
      "source": [
        "# Using a pre-trained model\n",
        "A common alternative to train a model from scratch consists in using a pre-trained model.<br>\n",
        "The idea is to replace the convolutional part with a highly optimized convolutional part engineered and trained previously by someone else.<br>\n",
        "Usually the models that we can use through keras.applications have been trained over the ImageNet dataset. <br>\n",
        "Today we are going to use the Xception Net model. Its architecture it is shown below:\n",
        "<img src=\"https://drive.google.com/uc?id=1eKivBCSKnWKyBxmGe5s64oOyhzhuCaqU\" width=\"600px\"><br>\n",
        "After the convolutional part replacement we still need to set up a fully connected part.<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc8dz23k2iWR"
      },
      "outputs": [],
      "source": [
        "# Creating the model based over the pretrained Xception network\n",
        "from tensorflow.keras import applications\n",
        "import tensorflow\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(tensorflow.keras.layers.UpSampling2D(size=(7,7),input_shape=(32,32,3)))\n",
        "\n",
        "Xception_model = applications.Xception(weights = \"imagenet\", include_top=False, input_shape = (224, 224, channels))\n",
        "\n",
        "for layer in Xception_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "Inputs = layers.Input(shape=(32,32,3))\n",
        "x = model(Inputs)\n",
        "x = Xception_model(x)\n",
        "x = layers.Flatten()(x)\n",
        "# let's add a fully-connected layer\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "# and a logistic layer for 10 classes\n",
        "predictions = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "pre_trained_model = tensorflow.keras.Model(Inputs, outputs=predictions)\n",
        "pre_trained_model.compile(loss=tensorflow.keras.losses.categorical_crossentropy, optimizer=tensorflow.keras.optimizers.Adam(),metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyHEx6bS2iWU"
      },
      "outputs": [],
      "source": [
        "# Visualize the model through the summary function\n",
        "pre_trained_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1Dkp9ix2iWX"
      },
      "outputs": [],
      "source": [
        "# Let's train the model!\n",
        "epochs = 5 #it may take a while, the number of epochs should be low...\n",
        "pretrained_model_history = pre_trained_model.fit(x_train, y_train_enc, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val_enc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJaZdXQT2iWb"
      },
      "outputs": [],
      "source": [
        "# Getting the results\n",
        "plot_history(pretrained_model_history)\n",
        "\n",
        "print(\"Training accuracy: \", accuracy_score(np.argmax(pre_trained_model.predict(x_train), axis=-1), y_train))\n",
        "print(\"Validation accuracy: \", accuracy_score(np.argmax(pre_trained_model.predict(x_val), axis=-1), y_val))\n",
        "print(\"Test accuracy: \", accuracy_score(np.argmax(pre_trained_model.predict(x_test), axis=-1), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_2_LXsy2iWg"
      },
      "source": [
        "# Comparing the models\n",
        "\n",
        "Now that we trained both the \"from scratch\" and the \"pre-trained\" models, we are going to compare the obtained results obtained during the training. We are going to consider accuracy and loss.<br>\n",
        "**What can you expect from these plots?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vovtvdUg2iWh",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Create here the plots to compare the \"from scratch\" model and the \"pretrained\" model\n",
        "# Try to produce a comparison plot about the accuracies (train and validation) and another plot for the losses\n",
        "# Creating the plots to compare the \"from scratch\" model and the \"pretrained\" model\n",
        "# Producing accuracy over epochs plot\n",
        "\n",
        "scratch_model_train_acc = scratch_model_history.history['accuracy']\n",
        "scratch_model_valid_acc = scratch_model_history.history['val_accuracy']\n",
        "scratch_model_train_loss = scratch_model_history.history['loss']\n",
        "scratch_model_valid_loss = scratch_model_history.history['val_loss']\n",
        "\n",
        "pretrained_model_train_acc = pretrained_model_history.history['accuracy']\n",
        "pretrained_model_valid_acc = pretrained_model_history.history['val_accuracy']\n",
        "pretrained_model_train_loss = pretrained_model_history.history['loss']\n",
        "pretrained_model_valid_loss = pretrained_model_history.history['val_loss']\n",
        "\n",
        "print(\"Producing accuracy over epochs plot\")\n",
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(16,7))\n",
        "\n",
        "plt.plot(scratch_model_train_acc, label=\"Scratch Train Acc.\")\n",
        "plt.plot(scratch_model_valid_acc, label=\"Scratch Valid. Acc.\")\n",
        "\n",
        "plt.plot(pretrained_model_train_acc, label=\"Pre-Trained Train Acc.\")\n",
        "plt.plot(pretrained_model_valid_acc, label=\"Pre-Trained Valid. Acc.\")\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy(%)')\n",
        "plt.legend(loc='lower right', fancybox=True, shadow=True, ncol=4)\n",
        "plt.grid()\n",
        "plt.savefig('acc_epochs.png', dpi=300)\n",
        "\n",
        "\n",
        "# Producing loss over epochs plot\n",
        "print(\"Producing loss over epochs plot\")\n",
        "fig = plt.figure(figsize=(16,7))\n",
        "\n",
        "plt.plot(scratch_model_train_loss, label=\"Scratch Train Loss\")\n",
        "plt.plot(scratch_model_valid_loss, label=\"Scratch Valid. Loss\")\n",
        "\n",
        "plt.plot(pretrained_model_train_loss, label=\"Pre-Trained Train Loss\")\n",
        "plt.plot(pretrained_model_valid_loss, label=\"Pre-Trained Valid. Loss\")\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right', fancybox=True, shadow=True, ncol=4)\n",
        "plt.grid()\n",
        "plt.savefig('loss_epochs.png', dpi=300)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bszQNCEz2iWq"
      },
      "source": [
        "**What information can you get from these plots?**<br>\n",
        "**Try to visualize the differences between the deeper model and the pre-trained xception model!\n",
        "**Are they showing what you expected?**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}