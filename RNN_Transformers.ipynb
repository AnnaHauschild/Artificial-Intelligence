{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnaHauschild/Artificial-Intelligence/blob/main/RNN_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h-pNS_KaIbZ"
      },
      "source": [
        "# Lab 4: Deep Sequence Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5WGdTkeLOzp"
      },
      "source": [
        "The forth lab session is about data that have a sequential structure that must be taken into account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TNAZGt4aIbe"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "import os, json, re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnmA6CbEaIbv"
      },
      "source": [
        "# 4.1 Deal with sequential data\n",
        "<img src=\"https://drive.google.com/uc?id=1k6HwtWlMTkVJFuyBpeBGF2sApqxR-KRc\" width=\"600px\" align=\"right\"><br>\n",
        "In this lab we see Deep Learning models that can process sequential data (text, timeseries,..).<br>\n",
        "These models don’t take as input raw text: they only work with numeric tensors; **vectorizing** text is the process of transforming text into numeric tensors.<br><br><br>\n",
        "The different units into which you can break down text (words, characters) are called tokens; then if you apply a tokenization scheme, you associate numeric vectors with the generated tokens.<br>\n",
        "These vectors, packed into sequence tensors, are fed into Deep Neural Network.<br>\n",
        "There are multiple ways to associate a vector with a token: we will see One-Hot Encoding and Token Embedding.<br>\n",
        "In this section we are going to deal with:\n",
        "* 3.1.1 One-Hot Encoding\n",
        "* 3.1.2 Word embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhRTCGvGaIbz"
      },
      "source": [
        "## 4.1.1 One-Hot Encoding\n",
        "One-Hot Encoding consists of associating a unique integer index with every word and then turning this integer index $i$ into a binary vector of size $N$ (the size of the vocabulary); the vector is all zeros except for the $i$-th entry, which is 1.\n",
        "<img src=\"https://drive.google.com/uc?id=1OzK9t_WXQsaDuZoOTQSksLuNMubXm0gc\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8hfjPNJaIb1"
      },
      "source": [
        "#### Try to perform One-Hot Encoding using Tokenizer\n",
        "Keras provides the Tokenizer class for preparing text documents for DL.<br>\n",
        "The Tokenizer must be constructed and then fit on either raw text documents or integer encoded text documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EPyke25aIb3"
      },
      "outputs": [],
      "source": [
        "# define 4 documents\n",
        "docs = ['Well done!','Good work','Great effort','nice work']\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(docs)\n",
        "\n",
        "\n",
        "encoded_docs = tokenizer.texts_to_matrix(docs, mode='count')\n",
        "print(encoded_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDycrywyaIb-"
      },
      "source": [
        "Some problems related to this kind of encoding are sparsity of the solution and the high dimensionality of the vector encoding of the tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHeXmpPfaIcB"
      },
      "source": [
        "## 4.1.2 Word embedding\n",
        "<img src=\"https://drive.google.com/uc?id=1YRcQ1_5n8Qay0GFoSLVrNcEKWeik5G3S\" width=\"400px\" align=\"right\"><br>\n",
        "The vector obtained from word embedding is dense and has lower dimensionality w.r.t One-Hot Encoding vector; the dimensionality of embedding space vector is an hyperparameter.<br>\n",
        "There are two ways to obtain word embeddings:<br>\n",
        "* May be learned jointly with the network\n",
        "* May use pre-trained word vectors (Word2Vec, GloVe,..)\n",
        "\n",
        "\n",
        "Word embeddings maps human language into a geometric space; in a reasonable embedding space synonyms are embedded into similar word vectors and the geometric distance between any two word vectors reflects the semantic distance between the associated words (words meaning different things are embedded at points far away from each other, whereas related words are closer).<br>\n",
        "How good is a word-embedding space depends on the specific task.<br>\n",
        "It is reasonable to learn a new embedding space with every new task: with backpropagation and Keras it reduces to learn the weights of the Embedding layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFHFL8wSaIcF"
      },
      "source": [
        "### Learning Word Embeddings with the embedding layer\n",
        "#### Load imdb dataset\n",
        "This dataset contains movies reviews from IMDB, labeled by sentiment(positive/negative); reviews have been preprocessed, and each review is encoded as a sequence of word indexes(integers).<br>\n",
        "https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOxdESRALOzv"
      },
      "outputs": [],
      "source": [
        "max_features = 10000\n",
        "maxlen = 50\n",
        "batch_size = 32\n",
        "embedding_dim = 16\n",
        "epochs = 10\n",
        "\n",
        "imdb = tf.keras.datasets.imdb\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5muY3QYaIcK"
      },
      "source": [
        "#### Show the size of vocabulary and the most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXB3wghhaIcL"
      },
      "outputs": [],
      "source": [
        "word_to_index = imdb.get_word_index()\n",
        "\n",
        "vocab_size = len(word_to_index)\n",
        "print('Vocab size : ', vocab_size)\n",
        "\n",
        "\n",
        "words_freq_list =words_freq_list = []\n",
        "\n",
        "for (k,v) in imdb.get_word_index().items():\n",
        "    words_freq_list.append((k,v))\n",
        "\n",
        "sorted_list = sorted(words_freq_list, key=lambda x: x[1])\n",
        "\n",
        "print(\"50 most common words: \\n\")\n",
        "print(sorted_list[0:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27uI4g4saIcQ"
      },
      "outputs": [],
      "source": [
        "word_to_index['otherwise']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXjwll2WaIda"
      },
      "source": [
        "# 4.2 Recurrent Neural Network\n",
        "Here https://colah.github.io/posts/2015-08-Understanding-LSTMs/ you can find a clear explanation about RNNs and LSTMs; the following is a summary of the main concepts.\n",
        "\n",
        "\n",
        "A major characteristic of some neural networks, as ConvNet, is that they have no memory: each input is processed independently, with no state kept in between inputs. Biological intelligence processes information incrementally while maintaining an internal model of what it’s processing, built from past information and constantly updated as new information comes in.<br>\n",
        "A recurrent neural network (RNN) adopts the same principle but in an extremely simplified version: it processes sequences by iterating through the sequence elements and maintaining a state containing information relative to what it has seen so far.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1RaDXXygc0HJv6YyIAjU4_Nbw1bXzAhAJ\" width=\"650px\"><br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Each input $x_{i=t-1, t, t+1, ..}$ is combined with the internal state and then is applied an activation function (e.g. $tanh$); then the output is computed $h_{i=t-1, t, t+1, ..}$ and the internal state is updated.<br>\n",
        "In many cases, you just need the last output ($h_{i=last t}$ at the end of the loop), because it already contains information\n",
        "about the entire sequence.\n",
        "<img src=\"https://drive.google.com/uc?id=1RtulDLKQnzZTSbBsD2n7TIlRVEaESB8o\" width=\"550px\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL10YVWTaIdi"
      },
      "source": [
        "#### Create the model\n",
        "In the following sections we will develop different models. Be careful to the fact that we are dealing with a binary classification problem!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PCby93EaIdi"
      },
      "outputs": [],
      "source": [
        "simple_lstm_model = tf.keras.Sequential()\n",
        "simple_lstm_model.add(tf.keras.layers.Embedding(max_features, 16))\n",
        "\n",
        "# Complete the model, it should be made by at least:\n",
        "# 1 SimpleRNN layer (go and look at the Tensorflow/Keras documentation!!)\n",
        "simple_lstm_model.add(tf.keras.layers.SimpleRNN(32))  # You can specify the number of units, here 32 is an example\n",
        "# 1 Dense layer\n",
        "simple_lstm_model.add(tf.keras.layers.Dense(1)) # Since it's for binary classification, 1 output unit\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "simple_lstm_model.compile(optimizer='rmsprop', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['acc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JTBSi5TaIdp"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DamvGVOpaIdq"
      },
      "outputs": [],
      "source": [
        "# Train your model here - use the fit() method\n",
        "history = simple_lstm_model.fit(x_train, y_train,epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5vtqdoL8gQl"
      },
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper left')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pt-M_A7OaIds"
      },
      "outputs": [],
      "source": [
        "#plot_history(history)\n",
        "print(history.history.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pZ5tcAjaIdx"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDbsgw_5aIdy"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = simple_lstm_model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test accuracy: %.3f, Test loss: %.3f' % (test_acc,test_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is6FCRVRaId0"
      },
      "source": [
        "#### Try to build a new model where you stack several recurrent layers.\n",
        "In such a setup, you have to get all of the intermediate layers to return full sequence of outputs. This is needed to return batch size, timesteps, hidden state. By doing this the output should contain all historical generated outputs along with time stamps (3D). This way the next layer can work further on the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eExHNhMraId1"
      },
      "outputs": [],
      "source": [
        "# Build the model. It should be made by at least:\n",
        "# 1 Embedding layer\n",
        "# More than 1 SimpleRNN layer, do not forget to put the return_sequences parameter to True\n",
        "# 1 Dense layer\n",
        "lstm_model = tf.keras.Sequential()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJlV5HynaId_"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWm__m8RaIeC"
      },
      "outputs": [],
      "source": [
        "# Train your model here\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNdh2-0haIeI"
      },
      "outputs": [],
      "source": [
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEdVU-XYaIeL"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNerI-XraIeM"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = lstm_model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test accuracy: %.3f, Test loss: %.3f' % (test_acc,test_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7VE3DnuLOz0"
      },
      "source": [
        "**What can you say about the obtained results? What about the comparison between these results and the ones obtained in the single layer RNN?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdJA3ttvaIeP"
      },
      "source": [
        "# 4.3 Transformers\n",
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-768x1082.png\" width=\"250px\" align=\"right\"><br>\n",
        "\n",
        "One main drawback about RNNs is their capacity to remember long-term dependencies. To alleviate this problem different models have been proposed, like Long Short Term Memories (LSTM) and Transformers.<br>\n",
        "*Transformers* is one of the best available model nowadays to deal with different kind of data (text, images..) and obtain state of the art results.\n",
        "\n",
        "The key component in the Transformer architecture is the Attention layer, that helps the encoder look at other words in the input sentence as it encodes a specific word. The attention mechanism, in theory, and given enough compute resources, have a wider window to reference from, therefore being capable of using the **entire context** of the text.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0KJgSymaIeR"
      },
      "source": [
        "#### Create Transformer model in TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHVtSQFzahRe"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px6QAO7jac2u"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d15KAMKpaIeR"
      },
      "outputs": [],
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "# Build the model called transformer_model. It should be made by at least:\n",
        "# 1 TokenAndPositionEmbedding layer with maxlen, vocab_size and embed_dim as arguments (we created them above!)\n",
        "# 1 TransformerBlock layer with embed_dim, num_heads and ff_dim as arguments\n",
        "# 1 GlobalAveragePooling1D layer\n",
        "# 2 Dense layers\n",
        "\n",
        "# tranformer_model=\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPz5KvjAaIeV"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6QTdfzZaIeW"
      },
      "outputs": [],
      "source": [
        "# Train your model here\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XELeM5_KaIef"
      },
      "outputs": [],
      "source": [
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPSLRTciaIei"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovW6sEhNaIej"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = tranformer_model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test accuracy: %.3f, Test loss: %.3f' % (test_acc,test_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S0X9OvrLOz1"
      },
      "source": [
        "# 4.4 Reuters newswire classification dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTdZTloBLOz1"
      },
      "source": [
        "The reuters newswire classification dataset is a dataset of 11,228 newswires from Reuters, labeled over 46 topics. More information about the dataset and how to use it can be found here:\n",
        "https://keras.io/api/datasets/reuters/\n",
        "\n",
        "Try to build a new model dealing with this new dataset.\n",
        "Try to use both the RNN and the Transfomrers approach, and select the best of them. What do you expect will be the best? Be carefull that this domain shift will imply some changes in your code as it is not a binary classification problem anymore!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNDQjfiLLOz1"
      },
      "outputs": [],
      "source": [
        "imdb = tf.keras.datasets.imdb\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.reuters.load_data(num_words=max_features)\n",
        "\n",
        "num_classes = np.max(y_train) + 1\n",
        "print(num_classes)\n",
        "\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYobCNa4LOz1"
      },
      "source": [
        "#### Create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LCRuwvhLOz1"
      },
      "outputs": [],
      "source": [
        "# Build the model. Suggestion: use SparseCategoricalCrossentropy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCkqhttLLOz1"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0W_A5xpALOz2"
      },
      "outputs": [],
      "source": [
        "# Train your model here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDknvnMsLOz2"
      },
      "outputs": [],
      "source": [
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMsHGlydLOz2"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wljwh3tLOz2"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test accuracy: %.3f, Test loss: %.3f' % (test_acc,test_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z1yy-ocLOz2"
      },
      "source": [
        "**Comment the results!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzENrsHMC4Fw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}